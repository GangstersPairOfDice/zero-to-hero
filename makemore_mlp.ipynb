{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82bd27-bda1-4979-b420-10bf3ef9af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tinygrad import Tensor\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['GPU'] = '1' # set to tinygrad backend to GPU since METAL doesn't work on older intel macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010fd07-1363-4850-a389-f42970e653e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean pokemon name data\n",
    "\n",
    "import csv\n",
    "\n",
    "names = []\n",
    "\n",
    "with open('Pokemon_moves.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader) # skips header row\n",
    "    for row in reader:\n",
    "        if len(row) > 1:\n",
    "            names.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242e3fe-2e3b-49dc-8191-0260772779ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9179353-1daa-46d9-892f-864e442ee2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdca707-561f-46c2-b29e-01b55b30c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabular of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "# stoi = string to int, itos = int to string\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['|'] = 0 # set | as end char, since all other end chars are already used\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)\n",
    "\n",
    "# finding # of unique chars so we can set our Tensor dim. later\n",
    "all_chars = set(''.join(stoi))\n",
    "num_unique_chars = len(all_chars)\n",
    "\n",
    "print('num_unique_chars =', num_unique_chars)\n",
    "print('all_chars = ', all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aca16d-3a4d-440a-874a-caf05157d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many chars do we take to predict the next on?\n",
    "X, Y = [], [] # X = inputs, Y = labels\n",
    "\n",
    "for n in names:\n",
    "\n",
    "    #print(n)\n",
    "    context = [0] * block_size # start with padded context\n",
    "\n",
    "    # iter over all chars\n",
    "    for ch in n + '|':\n",
    "        ix = stoi[ch] # get char in sequence\n",
    "        X.append(context) # stores current running context\n",
    "        Y.append(ix) # stores current char\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append (rolling window of context)\n",
    "\n",
    "X = Tensor(X)\n",
    "Y = Tensor(Y)\n",
    "\n",
    "# build the dataset ( in terms of train, val, and test sets )\n",
    "def build_dataset(names):\n",
    "    block_size = 3 # context length: how many chars do we take to predict the next on?\n",
    "    X, Y = [], [] # X = inputs, Y = labels\n",
    "    \n",
    "    for n in names:\n",
    "    \n",
    "        #print(n)\n",
    "        context = [0] * block_size # start with padded context\n",
    "    \n",
    "        # iter over all chars\n",
    "        for ch in n + '|':\n",
    "            ix = stoi[ch] # get char in sequence\n",
    "            X.append(context) # stores current running context\n",
    "            Y.append(ix) # stores current char\n",
    "            #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix] # crop and append (rolling window of context)\n",
    "    \n",
    "    X = Tensor(X)\n",
    "    Y = Tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n1])\n",
    "Xdev, Ydev = build_dataset(names[n1:n2])\n",
    "Xte, Yte = build_dataset(names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b2019-5146-4d9c-9666-ac56f4bed8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset shape\n",
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89167c6e-874c-4b74-a179-8bd842b94147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building embedding lookup table\n",
    "# we try cramming everything into two dimensional space\n",
    "\n",
    "C = Tensor.randn((num_unique_chars, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18d618-de16-4d15-8a57-868b9d331403",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9ed59-289b-45f3-b998-1200c3ae4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### hidden layer (3x2 = num inputs to this layer (context x embedding dim), and arb. 100 neurons)\n",
    "W1 = Tensor.randn(6, 100)\n",
    "b1 = Tensor.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16c227-29ec-49c7-8afa-3349528751b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the .view(-1) infers the rest of the shape of the viewed tensor\n",
    "h = (emb.view(-1,6) @ W1 + b1).tanh()\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03b351-892b-45f8-94b9-eeaaa6e57645",
   "metadata": {},
   "outputs": [],
   "source": [
    "### output layer\n",
    "\n",
    "W2 = Tensor.randn(100,64)\n",
    "b2 = Tensor.randn(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27813080-d024-4c04-a96a-d0446c27d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f57be6-e67b-4123-a472-f70eaacea0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c9ddc-6d20-47ea-b73c-38e96e5b4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91606ce-c84d-4866-8b0c-657ec5092027",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdim=True) # normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46053824-14b2-4528-96a6-05acb46012c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819b9b1-b26a-4c97-8318-b09394016fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "-prob[Tensor.arange(60), Y].log().mean().numpy() # neg log likelyhood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582b4ba-4583-4b68-be3f-e44eea7bc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- cleanup time -------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59747a4-4e60-4858-aa0c-a43b5e233279",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr.shape, Ytr.shape # dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d34d3-9256-4437-91de-44c1fadd17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Tensor.randn(64, 5)\n",
    "W1 = Tensor.randn(15, 100)\n",
    "b1 = Tensor.randn(100)\n",
    "W2 = Tensor.randn(100, 64)\n",
    "b2 = Tensor.randn(64)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d38caa-1266-41c6-9cc6-dc332ca6637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in parameters) # numb of params in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9b35b-ca7c-45f6-becd-cce32bab45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = Tensor.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76844f6c-d10e-4fbd-9b94-bbfa52bebb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a20206-b2c4-4248-bbf6-c74b307c1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fa14b-5fd6-4662-b6b2-c8d8e2cb379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "import time\n",
    "\n",
    "lr = 0.01413\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = Tensor.randint(32, low=0, high=Xtr.shape[0], requires_grad=False)\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 10)\n",
    "    h = (emb.view(-1, 15) @ W1 + b1).tanh()  # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 64)\n",
    "\n",
    "    # compute loss\n",
    "    loss = Tensor.cross_entropy(logits, Ytr[ix])\n",
    "\n",
    "    # zero gradients\n",
    "    for p in parameters:\n",
    "        p.grad = p.zeros_like()\n",
    "\n",
    "    # backwards pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.replace(p.add(-lr * p.grad))\n",
    "\n",
    "    # print loss\n",
    "    print(f'step {i}: {loss.item()}') # we print here since forward is lazy, \n",
    "    #                  # doesnt get realized until .backward is called\n",
    "    \n",
    "    #lrs[i]\n",
    "\n",
    "    # track stats\n",
    "    #lri.append(lre[i].item())\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317af67-e17a-49d2-a939-6fbbc5c0b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53042dd4-5db4-4d4b-984f-693b61c44e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss over training set\n",
    "\n",
    "emb = C[Xtr] # (60, 3, 2)\n",
    "h = (emb.view(-1,15) @ W1 + b1).tanh() # (60, 100)\n",
    "logits = h @ W2 + b2 # (60, 64)\n",
    "#counts = logits.exp()\n",
    "#prob = counts / counts.sum(1, keepdim=True) # normalizing\n",
    "#loss = -prob[Tensor.arange(60), Y].log().mean()\n",
    "loss = Tensor.cross_entropy(logits, Ytr)\n",
    "print(f'empirical loss: {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7597c-4e7b-4ea6-a6c0-4b64b9fbbbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss over dev test\n",
    "\n",
    "emb = C[Xdev] # (60, 3, 2)\n",
    "h = (emb.view(-1,15) @ W1 + b1).tanh() # (60, 100)\n",
    "logits = h @ W2 + b2 # (60, 64)\n",
    "#counts = logits.exp()\n",
    "#prob = counts / counts.sum(1, keepdim=True) # normalizing\n",
    "#loss = -prob[Tensor.arange(60), Y].log().mean()\n",
    "loss = Tensor.cross_entropy(logits, Ydev)\n",
    "loss.realize()\n",
    "print(f'empirical loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ae19f-2f2e-4b58-85a8-eacf36f44877",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data(), C[:,1].data(), s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha='center', va='center', color=\"white\")\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8bb64-103e-441d-a320-f54b9211718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training split, dev/validation split, test split\n",
    "# 80%, 10%, 10% of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a530e-52cd-4793-8acb-76c53666dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ||| starting chars\n",
    "    while True:\n",
    "        emb = C[Tensor(context)] # (1, block_size, d)\n",
    "        h = (emb.view(1, -1) @ W1 + b1).tanh()\n",
    "        logits = h @ W2 + b2\n",
    "        probs = logits.softmax(axis=1)\n",
    "        ix = probs.multinomial(num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee3203-4ea6-467d-b5d4-adb151ad9332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
